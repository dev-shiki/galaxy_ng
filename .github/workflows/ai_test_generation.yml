name: AI-Powered Test Generation

on:
  workflow_dispatch:
    inputs:
      limit:
        description: 'Maximum number of modules to generate tests for'
        required: false
        default: '5'
        type: number
      model:
        description: 'SambaNova model to use'
        required: false
        default: 'Meta-Llama-3.1-8B-Instruct'
        type: string
      create_pr:
        description: 'Create PR with generated tests'
        required: false
        default: true
        type: boolean
      ai_correction:
        description: 'Use AI-assisted error correction'
        required: false
        default: true
        type: boolean

jobs:
  analyze-coverage:
    name: Analyze Coverage and Generate Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better context
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
      
      - name: Install system dependencies
        run: |
          sudo apt update
          sudo apt install -y libsasl2-dev libldap2-dev libssl-dev gettext
      
      - name: Install dependencies
        run: |
          # Install core requirements
          python -m pip install --upgrade pip wheel setuptools
          pip install numpy
          pip install tox pytest pytest-django pytest-cov

          # Install required packages for the scripts
          pip install requests

          # Install the package itself in development mode
          pip install -e .
      
      - name: Create test infrastructure
        run: |
          # Create a mock pulp_smash config to avoid errors
          mkdir -p ~/.config/pulp_smash
          cat << EOF > ~/.config/pulp_smash/settings.json
          {
              "pulp": {
                  "auth": ["admin", "admin"],
                  "version": "3.0",
                  "selinux enabled": false
              },
              "hosts": [
                  {
                      "hostname": "localhost",
                      "roles": {
                          "api": {"port": 24817, "scheme": "http", "service": "nginx"},
                          "content": {"port": 24816, "scheme": "http", "service": "pulp_content_app"},
                          "pulp resource manager": {},
                          "pulp workers": {},
                          "redis": {},
                          "shell": {"transport": "local"},
                          "squid": {}
                      }
                  }
              ]
          }
          EOF

          # Create pytest.ini to configure pytest
          cat > pytest.ini << EOF
          [pytest]
          testpaths = galaxy_ng
          python_files = test_*.py
          python_classes = Test*
          python_functions = test_*
          addopts = -p no:pulp_ansible -p no:pulpcore -p no:pulp_smash
          EOF

          # Create directory structure for generated tests
          mkdir -p galaxy_ng/tests/unit/ai_generated/
          touch galaxy_ng/tests/unit/ai_generated/__init__.py
          touch galaxy_ng/tests/__init__.py
          touch galaxy_ng/tests/unit/__init__.py
      
      - name: Run original tests
        run: |
          # Run tests with tox as it was working before
          tox --colored yes -e py311 || true
          
          # Make a backup of the original coverage.xml
          if [ -f coverage.xml ]; then
            cp coverage.xml original_coverage.xml
          else
            echo "<?xml version='1.0' encoding='UTF-8'?><coverage></coverage>" > original_coverage.xml
          fi
      
      - name: Analyze coverage for test candidates
        run: |
          python .github/scripts/coverage_analyzer.py original_coverage.xml test_candidates.json
      
      - name: Set up SambaNova API Key
        run: |
          echo "SAMBANOVA_API_KEY=${{ secrets.SAMBANOVA_API_KEY }}" >> $GITHUB_ENV
      
      - name: Generate tests with AI
        run: |
          mkdir -p generated_tests
          python .github/scripts/ai_test_generator.py \
            --candidates test_candidates.json \
            --limit ${{ github.event.inputs.limit }} \
            --model ${{ github.event.inputs.model }} \
            --output-dir generated_tests
      
      - name: Run Test Fixes with AI-assisted correction
        if: ${{ github.event.inputs.ai_correction == 'true' }}
        run: |
          # Only run the test fixer if there are generated tests
          if [ -d "galaxy_ng/tests/unit/ai_generated" ] && [ "$(ls -A galaxy_ng/tests/unit/ai_generated)" ]; then
            echo "Fixing generated tests with AI assistance..."
            python .github/scripts/fix_test_files.py \
              --path galaxy_ng/tests/unit/ai_generated \
              --api-key "$SAMBANOVA_API_KEY" \
              --model "${{ github.event.inputs.model }}"
          else
            echo "No tests to fix."
          fi
        env:
          SAMBANOVA_API_KEY: ${{ secrets.SAMBANOVA_API_KEY }}
      
      - name: Run Test Fixes without AI assistance
        if: ${{ github.event.inputs.ai_correction != 'true' }}
        run: |
          # Only run the test fixer if there are generated tests
          if [ -d "galaxy_ng/tests/unit/ai_generated" ] && [ "$(ls -A galaxy_ng/tests/unit/ai_generated)" ]; then
            echo "Fixing generated tests with standard fixes..."
            python .github/scripts/fix_test_files.py --path galaxy_ng/tests/unit/ai_generated
          else
            echo "No tests to fix."
          fi
      
      - name: Validate Fixed Tests
        run: |
          # Create a simple script to check test files
          cat > check_tests.py << 'EOF'
          import os
          import sys
          import ast
          import re

          def check_file(filepath):
              print(f"Checking {filepath}...")
              try:
                  with open(filepath, 'r') as f:
                      content = f.read()
                  
                  # Check for proper Django setup
                  if "django.setup()" not in content:
                      print(f"WARNING: Django setup missing in {filepath}")
                  
                  # Check for factory imports
                  if re.search(r'from galaxy_ng\.[^\s]+\s+import\s+factories', content):
                      print(f"WARNING: Direct factory import in {filepath}")
                  
                  # Validate syntax
                  try:
                      ast.parse(content)
                      print(f"✅ {filepath} is syntactically valid")
                      return True
                  except SyntaxError as e:
                      print(f"❌ {filepath} has syntax error: {e}")
                      return False
              except Exception as e:
                  print(f"Error checking {filepath}: {e}")
                  return False

          # Check all test files in the directory
          test_dir = "galaxy_ng/tests/unit/ai_generated"
          valid_count = 0
          error_count = 0

          if os.path.exists(test_dir):
              for file in os.listdir(test_dir):
                  if file.endswith(".py") and file != "__init__.py":
                      file_path = os.path.join(test_dir, file)
                      if check_file(file_path):
                          valid_count += 1
                      else:
                          error_count += 1
              
              print(f"\nValidation results: {valid_count} valid, {error_count} with errors")
              sys.exit(1 if error_count > 0 else 0)
          else:
              print(f"Test directory {test_dir} not found")
              sys.exit(1)
          EOF

          # Run the checking script
          python check_tests.py || echo "Some tests need additional fixes"
      
      - name: Try running generated tests
        run: |
          # Create a proper conftest.py in the ai_generated directory with robust error handling
          cat > galaxy_ng/tests/unit/ai_generated/conftest.py << 'EOF'
          import os
          import sys
          import pytest
          from unittest import mock

          # Ensure mock is always available for test modules
          sys.modules['mock'] = mock

          # Setup Django environment with fallback mechanisms
          os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'galaxy_ng.settings')

          # Add project root to path
          project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../.."))
          if project_root not in sys.path:
              sys.path.insert(0, project_root)

          # Mock common modules that might be missing
          common_modules = [
              'galaxy_ng.app.management.commands.sync_galaxy_collections',
              'galaxy_ng.social.auth',
              'galaxy_ng.social.factories',
              'galaxy_ng.app.utils.roles',
              'galaxy_ng.app.access_control.access_policy'
          ]

          for module_path in common_modules:
              if module_path not in sys.modules:
                  parts = module_path.split('.')
                  # Create parent modules if they don't exist
                  for i in range(1, len(parts)):
                      parent = '.'.join(parts[:i])
                      if parent not in sys.modules:
                          sys.modules[parent] = mock.MagicMock()
                  # Create the module itself
                  sys.modules[module_path] = mock.MagicMock()

          # Create parent module for settings if needed
          if 'galaxy_ng' not in sys.modules:
              sys.modules['galaxy_ng'] = mock.MagicMock()

          # Try to set up Django, with fallback to mock settings
          try:
              import django
              django.setup()
          except ModuleNotFoundError:
              # Create a minimal mock settings module
              mock_settings = mock.MagicMock()
              mock_settings.INSTALLED_APPS = [
                  'django.contrib.auth',
                  'django.contrib.contenttypes',
                  'django.contrib.sessions',
                  'pulpcore',
                  'galaxy_ng'
              ]
              mock_settings.DATABASES = {
                  'default': {
                      'ENGINE': 'django.db.backends.sqlite3',
                      'NAME': ':memory:',
                  }
              }
              mock_settings.MIDDLEWARE = []
              mock_settings.TEMPLATES = []
              mock_settings.LOGGING_CONFIG = None
              mock_settings.LOGGING = {}
              mock_settings.SECRET_KEY = 'test-key'
              mock_settings.DEBUG = True
              mock_settings.ALLOWED_HOSTS = ['*']
              
              # Add the mock settings to sys.modules
              sys.modules['galaxy_ng.settings'] = mock_settings
              
              # Mock Django packages that might be needed
              django_modules = [
                  'django.contrib.auth.models',
                  'django.contrib.contenttypes.models',
                  'django.db.models',
                  'django.core.exceptions'
              ]
              
              for module_path in django_modules:
                  if module_path not in sys.modules:
                      parts = module_path.split('.')
                      for i in range(1, len(parts)):
                          parent = '.'.join(parts[:i])
                          if parent not in sys.modules:
                              sys.modules[parent] = mock.MagicMock()
                      sys.modules[module_path] = mock.MagicMock()
              
              # Mock common model exceptions
              DoesNotExist = type('DoesNotExist', (Exception,), {})
              django.db.models.Model = mock.MagicMock()
              django.db.models.Model.DoesNotExist = DoesNotExist
              
              # Try again to set up Django with our mocks in place
              django.setup()
          
          # Common test fixtures
          @pytest.fixture
          def mock_request():
              return mock.MagicMock(
                  user=mock.MagicMock(
                      username="test_user",
                      is_superuser=False,
                      is_authenticated=True
                  )
              )
          
          @pytest.fixture
          def mock_superuser():
              return mock.MagicMock(
                  username="admin_user",
                  is_superuser=True,
                  is_authenticated=True
              )
          
          @pytest.fixture
          def mock_models():
              """Returns common mocked models for testing."""
              models = mock.MagicMock()
              
              # Create DoesNotExist exceptions for common models
              models.User.DoesNotExist = type('DoesNotExist', (Exception,), {})
              models.Group.DoesNotExist = type('DoesNotExist', (Exception,), {})
              models.Collection.DoesNotExist = type('DoesNotExist', (Exception,), {})
              models.Repository.DoesNotExist = type('DoesNotExist', (Exception,), {})
              
              return models
          EOF

          # Create __init__.py files to ensure package structure is recognized
          touch galaxy_ng/tests/unit/ai_generated/__init__.py
          touch galaxy_ng/tests/unit/__init__.py
          touch galaxy_ng/tests/__init__.py

          # Add a patch helper file to fix common import issues in generated tests
          cat > galaxy_ng/tests/unit/ai_generated/patch_imports.py << 'EOF'
          """Helper module to patch common import issues in AI-generated tests."""
          import os
          import sys
          from unittest import mock

          def patch_test_imports():
              """
              Patch common import problems in generated tests.
              Call this at the start of each test file to ensure proper mocking.
              """
              # Ensure unittest.mock is available as just 'mock'
              sys.modules['mock'] = mock
              
              # Mock common modules that might be referenced
              modules_to_mock = [
                  'galaxy_ng.app.management.commands.sync_galaxy_collections',
                  'galaxy_ng.social.auth',
                  'galaxy_ng.social.factories',
                  'galaxy_ng.app.utils.roles',
                  'galaxy_ng.app.access_control.access_policy',
                  'galaxy_ng.app.utils.galaxy',
                  'pulpcore',
                  'pulp_ansible'
              ]
              
              for module_path in modules_to_mock:
                  if module_path not in sys.modules:
                      # Create parent modules if they don't exist
                      parts = module_path.split('.')
                      for i in range(1, len(parts)):
                          parent = '.'.join(parts[:i])
                          if parent not in sys.modules:
                              sys.modules[parent] = mock.MagicMock()
                      # Create the module itself
                      sys.modules[module_path] = mock.MagicMock()
              
              # Add common factory mocks if factories isn't defined
              if 'factories' not in globals():
                  factories = mock.MagicMock()
                  factories.UserFactory = mock.MagicMock()
                  factories.GroupFactory = mock.MagicMock()
                  factories.NamespaceFactory = mock.MagicMock()
                  factories.CollectionFactory = mock.MagicMock()
                  sys.modules['factories'] = factories
          EOF

          # Try to patch any existing test files to make them work better
          for test_file in galaxy_ng/tests/unit/ai_generated/test_*.py; do
            # Skip if not a file
            [ ! -f "$test_file" ] && continue
            
            # Add import patch at the top of each test file
            sed -i '1s/^/# Import patch helper\nfrom galaxy_ng.tests.unit.ai_generated.patch_imports import patch_test_imports\npatch_test_imports()\n\n/' "$test_file"
            
            # Ensure unittest.mock is imported if mock.MagicMock is used
            if grep -q "mock\.MagicMock" "$test_file" && ! grep -q "from unittest import mock" "$test_file" && ! grep -q "import mock" "$test_file"; then
              sed -i '1s/^/from unittest import mock\n/' "$test_file"
            fi
          done

          # Attempt to collect tests to verify they can be imported
          python -m pytest galaxy_ng/tests/unit/ai_generated/ \
            --collect-only \
            -p no:pulp_ansible \
            -p no:pulpcore \
            -p no:pulp_smash \
            -v || echo "Some tests can't be collected"
            
      - name: Upload generated tests artifact
        uses: actions/upload-artifact@v4
        with:
          name: ai-generated-tests
          path: galaxy_ng/tests/unit/ai_generated/
          retention-days: 7
      
      - name: Upload test candidates artifact
        uses: actions/upload-artifact@v4
        with:
          name: test-candidates
          path: test_candidates.json
          retention-days: 7
      
      - name: Upload coverage artifact
        uses: actions/upload-artifact@v4
        with:
          name: coverage-data
          path: original_coverage.xml
          retention-days: 7
      
      - name: Create combined PR
        if: ${{ github.event.inputs.create_pr == 'true' }}
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "Add AI-Generated tests to improve coverage"
          title: "AI-Generated Tests"
          body: |
            This PR contains AI-generated tests to improve code coverage.
            
            The tests have been placed in the `galaxy_ng/tests/unit/ai_generated/` directory and have been preprocessed 
            to fix common issues like:
            - Non-existent factory imports
            - Hyphenated module names
            - Unclosed parentheses
            - Missing Django environment setup
            - Special module handling for __init__.py files
            
            Generated using:
            - Model: ${{ github.event.inputs.model }}
            - Modules analyzed: ${{ github.event.inputs.limit }}
            - AI-assisted correction: ${{ github.event.inputs.ai_correction }}
            
            No-Issue
          branch: ai-generated-tests-${{ github.run_id }}
          base: ${{ github.ref_name }}
          labels: |
            ai-generated
            testing
            automated-pr